{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "from PIL import Image\n",
    "import timm\n",
    "import torch\n",
    "import configparser\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from torchvision.utils import save_image\n",
    "from tqdm import tqdm\n",
    "import datetime \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('./config.ini')\n",
    "\n",
    "model_name = config['models']['MODEL'].strip()\n",
    "pretrained = config.getboolean('models', 'PRETRAINED')\n",
    "number_of_classes = int(config['models']['NUM_CLASSES'])\n",
    "dataset_path = config['data']['DATASET_PATH'].strip()\n",
    "train_split = float(config['training']['TRAIN_SPLIT'])\n",
    "test_split = float(config['training']['TEST_SPLIT'])\n",
    "val_split = float(config['training']['VAL_SPLIT'])\n",
    "learning_rate = float(config['training']['LR'])\n",
    "weight_decay = float(config['training']['WEIGHT_DECAY'])\n",
    "number_of_epochs = int(config['training']['N_EPOCHS'])\n",
    "batch_size = int(config['training']['BATCH_SIZE'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomViT(nn.Module):\n",
    "    def __init__(self, num_classes, pretrained=True, freeze_backbone=True):\n",
    "        super(CustomViT, self).__init__()\n",
    "        self.model = timm.create_model(\n",
    "            'vit_base_patch14_reg4_dinov2.lvd142m',\n",
    "            pretrained=pretrained,\n",
    "            num_classes=num_classes\n",
    "        )\n",
    "        if freeze_backbone:\n",
    "            for name, param in self.model.named_parameters():\n",
    "                if \"head\" not in name:\n",
    "                    param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 13   \n",
    "pretrained = True   \n",
    "model = CustomViT(num_classes=num_classes, pretrained=True, freeze_backbone=True)\n",
    "model_name = \"CustomViT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ColorJitter(\n",
    "        brightness=0.2, contrast=0.2, saturation=0.2, hue=0.02\n",
    "    ),\n",
    "    transforms.RandomResizedCrop(518, scale=(0.8, 1.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],  \n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    " \n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize(540),       \n",
    "    transforms.CenterCrop(518),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "def sanitize_filename(filename):\n",
    "    return \"\".join(c for c in filename if c.isalnum() or c in (' ', '.', '_')).rstrip()\n",
    "\n",
    " \n",
    "class ImageFolderWithPaths(datasets.ImageFolder):\n",
    "    def __getitem__(self, index):\n",
    "        original_tuple = super().__getitem__(index)\n",
    "        path = self.imgs[index][0]\n",
    "        return original_tuple + (path,)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return super().__len__()\n",
    "\n",
    " \n",
    "def save_classified_images(model, dataloader, device, output_dir, phase='test', class_names=None):\n",
    "    \"\"\"Save correctly classified and misclassified images with proper organization.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        dataloader: DataLoader with images and paths\n",
    "        device: Device to run inference on\n",
    "        output_dir: Base output directory\n",
    "        phase: 'val' or 'test' phase\n",
    "        class_names: List of class names for readable folder names\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model.eval()\n",
    "        phase_dir = os.path.join(output_dir, phase)\n",
    "        os.makedirs(phase_dir, exist_ok=True)\n",
    "        \n",
    "         \n",
    "        correct_dir = os.path.join(phase_dir, 'correct')\n",
    "        misclassified_dir = os.path.join(phase_dir, 'misclassified')\n",
    "        \n",
    "        for dir_path in [correct_dir, misclassified_dir]:\n",
    "            os.makedirs(dir_path, exist_ok=True)\n",
    "            if not os.path.exists(dir_path):\n",
    "                raise RuntimeError(f\"Failed to create directory: {dir_path}\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels, paths in tqdm(dataloader, desc=f\"Saving {phase} images\"):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                for img, label, pred, path in zip(images, labels, preds, paths):\n",
    "                    \n",
    "                    true_class = class_names[label.item()] if class_names else str(label.item())\n",
    "                    pred_class = class_names[pred.item()] if class_names else str(pred.item())\n",
    "                    \n",
    "                     \n",
    "                    filename = os.path.basename(path)\n",
    "                    save_name = f\"true_{true_class}_pred_{pred_class}_{filename}\"\n",
    "                    \n",
    "                     \n",
    "                    save_dir = correct_dir if label == pred else misclassified_dir\n",
    "                    save_path = os.path.join(save_dir, save_name)\n",
    "                    \n",
    "                    \n",
    "                    img = denormalize(img.cpu())\n",
    "                    save_image(img, save_path)\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving classified images: {e}\")\n",
    "        raise\n",
    "\n",
    "def denormalize(tensor):\n",
    "    \"\"\"Reverse the normalization applied to images with proper device handling.\"\"\"\n",
    "    device = tensor.device\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406], device=device).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225], device=device).view(3, 1, 1)\n",
    "    return tensor * std + mean\n",
    " \n",
    "full_dataset = ImageFolderWithPaths(\n",
    "    root=dataset_path, \n",
    "    transform=None   \n",
    ")\n",
    " \n",
    "if not os.path.exists(dataset_path):\n",
    "    raise ValueError(f\"Dataset path does not exist: {dataset_path}\")\n",
    "\n",
    "if not (0 < train_split < 1) or not (0 < val_split < 1) or not (0 < test_split < 1):\n",
    "    raise ValueError(\"Train/val/test splits must be between 0 and 1\")\n",
    "\n",
    "if train_split + val_split + test_split != 1.0:\n",
    "    print(\"Warning: Train/val/test splits don't sum to 1.0 - normalizing...\")\n",
    "    total = train_split + val_split + test_split\n",
    "    train_split /= total\n",
    "    val_split /= total\n",
    "    test_split /= total\n",
    "train_ratio = train_split\n",
    "val_ratio = val_split \n",
    "total_size = len(full_dataset)\n",
    "train_size = int(train_split * total_size)\n",
    "val_size = int(val_split * total_size)\n",
    "test_size = total_size - train_size - val_size  \n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    full_dataset, [train_size, val_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    " \n",
    "class TransformSubset(torch.utils.data.Dataset):\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img, label, path = self.subset[index]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label, path\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TransformSubset(train_dataset, transform=train_transform)\n",
    "val_dataset = TransformSubset(val_dataset, transform=eval_transform)\n",
    "test_dataset = TransformSubset(test_dataset, transform=eval_transform)\n",
    "\n",
    "\n",
    " \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n",
    "                         num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
    "                       num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
    "                        num_workers=4, pin_memory=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)  \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device info : \" ,device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"\n",
    "    Custom LR schedule:\n",
    "    - Epochs 0–1: 1e-4\n",
    "    - Epochs 2–3: 1e-5\n",
    "    - Epoch 4 onwards: 1e-6\n",
    "    \"\"\"\n",
    "    if epoch < 3:\n",
    "        new_lr = 1e-4\n",
    "    elif epoch < 5:\n",
    "        new_lr = 1e-4\n",
    "    else:\n",
    "        new_lr = 1e-4\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = new_lr\n",
    "    print(f\"Learning rate adjusted to: {new_lr:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, scheduler, scaler, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels, _ in train_loader: \n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate_model(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, _ in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            with autocast():\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc, all_preds, all_labels\n",
    "\n",
    "def test_model(model, test_loader, criterion, device, class_names):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, _ in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            with autocast():\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_loss = test_loss / total\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=class_names))\n",
    "    \n",
    "    return avg_loss, accuracy, all_preds, all_labels\n",
    "\n",
    "def start_training(model, train_loader, val_loader, criterion, optimizer, device, \n",
    "                  num_epochs, model_name, patience=5, base_lr=1e-4):\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    scheduler = OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=5e-5,\n",
    "        epochs=num_epochs,\n",
    "        steps_per_epoch=len(train_loader),\n",
    "        pct_start=0.1,\n",
    "        anneal_strategy='cos'\n",
    "    )\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accuracies, val_accuracies = [], []\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        print(f\"\\nEpoch [{epoch+1}/{num_epochs}] - Started at: {current_time}\")\n",
    "\n",
    "        train_loss, train_acc = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, scheduler, scaler, device\n",
    "        )\n",
    "        \n",
    "        val_loss, val_acc, val_preds, val_labels = validate_model(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "            best_model_state = model.state_dict()\n",
    "            \n",
    "            os.makedirs(\"./v1/models_output/VIT\", exist_ok=True)\n",
    "            torch.save({\n",
    "                'model_state_dict': best_model_state,\n",
    "                'class_names': full_dataset.classes,\n",
    "                'epoch': epoch + 1,\n",
    "                'val_loss': best_val_loss,\n",
    "                'train_accuracy': train_acc,\n",
    "                'val_accuracy': val_acc,\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict()\n",
    "            }, f\"./v1/models_output/VIT/best_{model_name}.pth\")\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            print(f\"No improvement for {epochs_without_improvement} epoch(s).\")\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Early stopping: no improvement in {patience} epochs.\")\n",
    "            break\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies\n",
    "\n",
    "def plot_loss_vs_epochs(train_losses, val_losses):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('loss_curve.png')\n",
    "    plt.show()\n",
    "\n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    num_epochs =50\n",
    "    patience = 10\n",
    "    \n",
    "     \n",
    "    class_names = full_dataset.classes\n",
    "    train_losses, val_losses, train_acc, val_acc = start_training(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        num_epochs=num_epochs,\n",
    "        model_name=model_name,\n",
    "        patience=patience\n",
    "    )\n",
    "    \n",
    "     \n",
    "    val_loss, val_acc, val_preds, val_labels = validate_model(\n",
    "        model, val_loader, criterion, device\n",
    "    )\n",
    "\n",
    "\n",
    "    print(f\"\\nFinal Validation Results:\")\n",
    "    print(f\"Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "     \n",
    "    save_classified_images(\n",
    "        model, val_loader, device,\n",
    "        'vit/classified_images',\n",
    "        phase='val',\n",
    "        class_names=class_names\n",
    "    )\n",
    "\n",
    "\n",
    "     \n",
    "    test_loss, test_acc, test_preds, test_labels = test_model(\n",
    "        model, test_loader, criterion, device, class_names\n",
    "    )\n",
    "    print(f\"\\nTest Results:\")\n",
    "    print(f\"Loss: {test_loss:.4f}, Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "   \n",
    "    save_classified_images(\n",
    "        model, test_loader, device,\n",
    "        'vit/classified_images',\n",
    "        phase='test',\n",
    "        class_names=class_names\n",
    "    )\n",
    "\n",
    "\n",
    "    \n",
    "    plot_loss_vs_epochs(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_vs_epochs(train_losses, val_losses):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('loss_curve.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(val_labels, val_preds)\n",
    "\n",
    "plt.figure(figsize=(13, 10))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "output_path = '../v1/models_output/revised_data/'\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "filename = f'{model_name}_confusion_matrix.png'\n",
    "plt.savefig(os.path.join(output_path, filename))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".bark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
