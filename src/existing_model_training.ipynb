{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms, models\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from torchvision.models import efficientnet_b4\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ResNet18(nn.Module):\n",
    "#     def __init__(self, num_classes=16, pretrained=False):\n",
    "#         super(ResNet18, self).__init__()\n",
    "#         self.model = models.resnet18(pretrained=pretrained)\n",
    "#         self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)\n",
    "#         self.model.conv1.weight.requires_grad = False  # Freezing the first layer weights\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)\n",
    "# class ResNet50(nn.Module):\n",
    "#             def __init__(self, num_classes=16, pretrained=False):\n",
    "#                 super(ResNet50, self).__init__()\n",
    "#                 self.model = models.resnet50(pretrained=pretrained)\n",
    "#                 self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)\n",
    "#                 self.model.conv1.weight.requires_grad = False\n",
    "\n",
    "#             def forward(self, x):\n",
    "#                 return self.model(x)\n",
    "# class ResNet34(nn.Module):\n",
    "#             def __init__(self, num_classes=16, pretrained=False):\n",
    "#                 super(ResNet34, self).__init__()\n",
    "#                 self.model = models.resnet34(pretrained=pretrained)\n",
    "#                 self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)\n",
    "#                 self.model.conv1.weight.requires_grad = False\n",
    "\n",
    "#             def forward(self, x):\n",
    "#                 return self.model(x)\n",
    "# class EfficientNetB0(nn.Module):\n",
    "#             def __init__(self, num_classes, pretrained=False):\n",
    "#                 super(EfficientNetB0, self).__init__()\n",
    "#                 self.model = EfficientNet.from_pretrained('efficientnet-b0') if pretrained else EfficientNet.from_name('efficientnet-b0')\n",
    "#                 self.model._fc = nn.Linear(self.model._fc.in_features, num_classes)\n",
    "\n",
    "#             def forward(self, x):\n",
    "#                 return self.model(x)\n",
    "# class NvidiaEfficientNetB4(nn.Module):\n",
    "#             def __init__(self, num_classes, pretrained=False):\n",
    "#                 super(NvidiaEfficientNetB4, self).__init__()\n",
    "#                 self.model = efficientnet_b4(weights='IMAGENET1K_V1' if pretrained else None)\n",
    "#                 self.model.classifier[1] = nn.Linear(self.model.classifier[1].in_features, num_classes)\n",
    "\n",
    "#             def forward(self, x):\n",
    "#                 return self.model(x)\n",
    "class VGG16(nn.Module):\n",
    "            def __init__(self, num_classes, pretrained=False):\n",
    "                super(VGG16, self).__init__()\n",
    "                self.model = models.vgg16(pretrained=pretrained)\n",
    "                \n",
    "                self.model.features[0].weight.requires_grad = False\n",
    "                self.model.features[0].bias.requires_grad = False\n",
    "                in_features = self.model.classifier[-1].in_features \n",
    "                self.model.classifier[-1] = nn.Linear(in_features, num_classes)\n",
    "\n",
    "            def forward(self, x):\n",
    "                return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bipp/BIPP/nCount/Bark Models/Barkdata-cnn-models/.bark/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/bipp/BIPP/nCount/Bark Models/Barkdata-cnn-models/.bark/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "num_classes = 16  # Adjust based on your use case\n",
    "model = VGG16(num_classes=num_classes, pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9846/2486704761.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrained_dict  = torch.load('../trained_models/prod/VGG16/VGG16.pth')\n"
     ]
    }
   ],
   "source": [
    "pretrained_dict  = torch.load('../trained_models/prod/VGG16/VGG16.pth')\n",
    "model_dict = model.state_dict()\n",
    "pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and \"fc\" not in k}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict.update(pretrained_dict)\n",
    "model.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "     transforms.RandomHorizontalFlip(),\n",
    "])\n",
    "\n",
    "full_dataset = datasets.ImageFolder(root='../CHR', transform=transform)\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.2\n",
    "batch_size = 64\n",
    "learning_rate=0.0001\n",
    "weight_decay=0.0001\n",
    "train_size = int(train_ratio * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)  \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device info : \" ,device)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def validate_model(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc, all_preds, all_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_training(num_epochs): \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\" started the epoch {epoch+1}/{num_epochs}\")\n",
    "        train_loss, train_acc = train_model(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc, val_preds, val_labels = validate_model(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.4f}\")\n",
    "        # print(f\"{val_labels,val_preds}\")\n",
    "\n",
    "\n",
    "start_training(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '../Barknet Trained Models/VGG16/VGG16.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "])\n",
    "\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')  \n",
    "    image = transform(image) \n",
    "    image = image.unsqueeze(0) \n",
    "    return image.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total  :  100 True Predictions: 88 False Predictions:  12 False Predictions :  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "false_predictions = []\n",
    "true_counts=0\n",
    "test_class=0\n",
    "for image in os.listdir(f'../test-data/{test_class}/'): \n",
    "    image_path = f'../test-data/{test_class}/' + image\n",
    "    image = preprocess_image(image_path)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)\n",
    "        _, predicted = outputs.max(1)\n",
    "        # print(outputs)\n",
    "        predicted_label = predicted.item()\n",
    "        if predicted_label == test_class :\n",
    "            true_counts+=1\n",
    "        else :\n",
    "            false_predictions.append(predicted_label)\n",
    "print('Total  : ', true_counts+ len(false_predictions), 'True Predictions:', true_counts, 'False Predictions: ', len(false_predictions), 'False Predictions : ', false_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_acc, val_preds, val_labels = validate_model(model, val_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(val_labels, val_preds)\n",
    "class_names = full_dataset.classes \n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".bark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
